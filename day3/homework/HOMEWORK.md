# RAG 実験レポート – HOMEWORK.md

## 1. 質問設計の観点と意図

| ID | 質問                             | タイプ/難易度          | 設計意図                                              |
| -- | ------------------------------ | ---------------- | ------------------------------------------------- |
| Q1 | 発明の新規性喪失の例外（特許法30条）の証明書提出期限    | **基本条文確認** / 易   | LLM 単体でも正答できる基礎知識でベースライン比較用                       |
| Q2 | 仮専用実施権が移転できる場合の列挙              | **複合列挙** / 中     | 条文を横断して複数条件を網羅的に取り出す必要があり、RAG が効果を発揮しやすい          |
| Q3 | 補償金請求権（特65条6項）の消滅時効と起算点        | **細部条文** / 難     | 民法準用条文を読み解く必要があり、RAG でも難易度が高い                     |
| Q4 | 存続期間延長登録の除外ケース（67条4項かっこ書き）     | **長大除外条件** / 難   | 複雑な条件文を正しく否定形で説明させる。コンテキストが散在しており retrieval 難度も高い |
| Q5 | 令和6年改正法における意匠 "グレースピリオド" 規定の有無 | **最新改正有無判断** / 中 | 参照 PDF に直接記載が無いネガティブ回答が必要。ハルシネーション検出狙い            |

> **ポイント**: 難易度と質問タイプを意図的にばらけさせ、RAG の *検索・根拠提示能力* と *ハルシネーション抑制効果* を可視化した。

---

## 2. RAG 実装方法と工夫点

| コンポーネント      | 設定値 / ライブラリ                                          | 工夫点                                               |
| ------------ | ---------------------------------------------------- | ------------------------------------------------- |
| **埋め込みモデル**  | `intfloat/multilingual-e5-large`                     | 法律用語を含む日本語 PDF への多言語適応力を評価                        |
| **LLM**      | `gemini‑2.5‑pro‑exp‑03‑25`                           | 温度 0.1 で determinism を確保しつつ 1024 token 出力許容       |
| **ドキュメント分割** | 500 文字チャンク / 200 重なり                                 | 条文内の条番号またぎを回避しつつ、引用部分が切れないサイズを手探りで選定              |
| **検索**       | FAISS + cosine・Top‑k=5                               | 質問当たり 5 件を取得し *reasoning* ウィンドウを確保                |
| **プロンプト**    | *retrieve‑then‑read* テンプレート (引用塊 + chain‑of‑thought) | コンテキストを番号付きで提示し、回答中に〈根拠 n〉を引用させるルールを明示            |
| **評価**       | ① Exact‑Match (自動) ② 4軸手動採点 (0‑5)                    | 手動軸=Accuracy / Completeness / Relevance / Clarity |

**工夫例**

* *Chunk overlap* を 200 に設定し、条文の途中切れによる retrieval miss を減少。
* `temperature=0.1` と **引用必須プロンプト** でハルシネーション抑制。
* 質問文を *日本語* で直接投げ、embedding との言語整合性を確保。

---

## 3. 結果分析

### 3.1 自動 Exact‑Match

| Q | Baseline | RAG | コメント                         |
| - | -------- | --- | ---------------------------- |
| 1 | ✅        | ❌   | 条文全文引用により文字列不一致扱い (実質内容は正確)  |
| 2 | ❌        | ❌   | 条件列挙質問のため EM は不利。内容は RAG が正解 |
| 3 | ❌        | ❌   | 両者とも回答生成失敗                   |
| 4 | ❌        | ❌   | 両者とも部分的説明で不一致                |
| 5 | ❌        | ❌   | ネガティブ回答なので EM 評価に不向き         |

> **示唆**: Exact‑Match は長文／列挙／否定問いには不向き。よりリッチな評価軸が必要。

### 3.2 手動評価 (平均スコア)

| Metric (0‑5) | Baseline Avg | RAG Avg |
| ------------ | ------------ | ------- |
| Accuracy     | **1.8**      | **3.4** |
| Completeness | 2.0          | 3.4     |
| Relevance    | 2.0          | 3.4     |
| Clarity      | 2.2          | 3.4     |

<small>※ 計算根拠: 5 問の個別スコアを単純平均。</small>

#### 改善が顕著だったケース

* **Q2**: Baseline 空欄 → RAG 5/5/5/5。条文 (34条の2) を適切に検索し列挙を完全網羅。
* **Q5**: Baseline 空欄 → RAG が「規定なし」を明確回答しハルシネーションを防止。

#### 失敗・未改善ケース

* **Q3**: 準用条文 (民法724条読み替え) を Retrieval できず両者とも回答不可。
* **Q4**: Retrieval は命中したもののプロンプトが肯定形説明用で、*「該当しない場合」* への反転ロジックが欠落。
* **Q1**: RAG は内容的には正確だが、EM 評価で不一致。フォーマット制約を満たす post‑processing が必要。

---

## 4. 考察

1. **RAG の有効性**

   * 平均 Accuracy +1.6pt 向上。検索可能な *明示的条文* については顕著に性能改善。
2. **限界**

   * 準用・読み替え条文など "implicit linkage" は embedding 検索がヒットしづらい。
   * 否定質問や除外条件のように *回答構造* が複雑なときは prompt design が課題。
3. **評価指標**

   * Exact‑Match は法務 QA には不適切。*Span‑level* F1 や *LLM‑based rubric* などが望ましい。

---

## 5. 発展的な改善案

* **知識グラフ併用**: 条文間の準用・参照関係をグラフ化し、検索時にリンク先ノードも展開する *Graph‑RAG* で implicit linkage を補完。
* **ネガティブ・クエリ対応 Prompt**: 「該当しない場合を答えよ」「規定は存在するか」など否定問いを明示的に扱うテンプレート分岐。
* **評価自動化**: GPT‑4o による *rubric scoring* (0‑5) を導入し、手動採点コストを削減。
* **再ランキング**: Retrieval 上位 20 → Cross‑Encoder (legal‑BERT) で再ランキングし、高精度 context を確保。
* **Chunk Tuning**: 条番号毎に YAML front‑matter を付与し "条単位" で indexing。チャンク切れ問題を根本解消。

---
